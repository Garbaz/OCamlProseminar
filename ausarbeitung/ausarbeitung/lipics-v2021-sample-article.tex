\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate]{lipics-v2021}

\usepackage{listings-rust}
%This is a template for producing LIPIcs articles. 
%See lipics-v2021-authors-guidelines.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling autoref support, use "autoref"
%for anonymousing the authors (e.g. for double-blind review), add "anonymous"
%for enabling thm-restate support, use "thm-restate"
%for enabling a two-column layout for the author/affilation part (only applicable for > 6 authors), use "authorcolumns"
%for producing a PDF according the PDF/A standard, add "pdfa"

%\pdfoutput=1 %uncomment to ensure pdflatex processing (mandatatory e.g. to submit to arXiv)
\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Approaches to Asynchronous Execution across Languages}

%\titlerunning{Dummy short title} %TODO optional, please use if title is longer than one line

\author{Tobias Hoffmann}{Albert-Ludwigs-Universit√§t Freiburg, Germany}{garbaz@t-online.de}{}{}

\authorrunning{T. Hoffmann} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Tobias Hoffmann} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\begin{CCSXML}
<ccs2012>
    <concept>
        <concept_id>10003752.10003753.10003761.10003762</concept_id>
        <concept_desc>Theory of computation~Parallel computing models</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
</ccs2012>
\end{CCSXML}
    
\ccsdesc[500]{Theory of computation~Parallel computing models}

\keywords{async, concurrency} %TODO mandatory; please add comma-separated list of keywords

\category{} %optional, e.g. invited paper

\relatedversion{} %optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversiondetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93]{Classification (e.g. Full Version, Extended Version, Previous Version}{URL to related version} %linktext and cite are optional

%\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
%\supplementdetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93, subcategory={Description, Subcategory}, swhid={Software Heritage Identifier}]{General Classification (e.g. Software, Dataset, Model, ...)}{URL to related version} %linktext, cite, and subcategory are optional

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

\acknowledgements{}%optional

\nolinenumbers %uncomment to disable line numbering



%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \EventEditors{John Q. Open and Joan R. Access}
% \EventNoEds{2}
% \EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
% \EventShortTitle{CVIT 2016}
% \EventAcronym{CVIT}
% \EventYear{2016}
% \EventDate{December 24--27, 2016}
% \EventLocation{Little Whinging, United Kingdom}
% \EventLogo{}
% \SeriesVolume{42}
% \ArticleNo{0}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\maketitle

%TODO mandatory: add short abstract of the document
\begin{abstract}
    In computing operations can be divided into either being CPU-bound or being IO-bound, the former being operations where the limiting factor to performance is the speed with which calculations can be performed, whereas the latter are operations where performance is limited by the responses of systems external to the CPU. While for both kinds of operations, concurrency is a commonly taken approach to achieve improvements in performance, the way this concurrency is systematically structured and implemented is quite different for the two applications. In the following, I will focus on the approaches provided by a selection of different languages to efficiently handle specifically IO-bound operations.
\end{abstract}

\section{Overview}
\label{sec:overview}

The primary distinction of IO-bound tasks is that they generally consist of a small amount of computation, while having relatively long wait times. This means that each IO-bound task does not require much CPU time in total, but importantly, whenever an IO-bound task does have to do computation, it is usually important for it to get the necessary CPU time quickly, so that it can dispatch another message and wait for a response from whatever external system it is interacting with.

So to handle a large number of such tasks simultaneously requires quick and efficient switching of tasks to fully utilize the processing resources available. However, ordinary threads as provided by most operating systems generally require too much time and resources to be created and deleted for them to be utilized directly. Therefore, most systems optimized for asynchronous execution of IO-bound tasks utilize a custom system that is abstracted over, or even entirely independent from, the concurrency systems provided by the underlying system architecture.

Another challenge in developing a system for asynchronous computation is the matter of a usable and safe abstract interface for programmers to interact with, due to it being such a fundamental change in how computation is organized inside a program. For this, two different approaches have developed, which are semantically similar, but conceptually and principally distinct.

One, generally aligned with the imperative programming paradigm, is to consider an asynchronous sequence of computation as one whole, which is then routinely interrupted and later resumed whenever it has to wait from some external asynchronous operation. This is usually implemented with bespoke features introduced into the language itself.

The other, generally aligned with the functional programming paradigm, is to instead consider an asynchronous sequence of computation to be made up of a chain of smaller asynchronous operations, resulting in a monadic structure where smaller asynchronous operations are composed into larger asynchronous operations.

While these two approaches look at the problem from different sides, in effect they result in the same semantic model: Bursts of computation delineated by waiting for external responses.

\section{OCaml}
\label{sec:ocaml}

While neither the OCaml compiler nor the standard library \lstinline{Base} provide any support for asynchronous computation, there are external libraries that introduce the functionality into the language. One is \lstinline{Async}, which comes with Jane Street's \lstinline{Core} library, another is \lstinline{Lwt}.

\paragraph*{Async}

Since the basic ideas of this library have already been covered in the presentation, their explanation will be kept brief here.

Async does not utilize system threads for concurrency, but rather implements it's own system of non-preemptive user-level threads to handle asynchronous operations \cite{Concurre6:online}. While this means that it does not benefit from the potential increase in performance from utilizing multiple cores, it does avoid the overhead that comes with creating and destroying system threads, while still allowing the user to avoid dead-time when waiting for external responses.

\paragraph*{Deferred}

For representing the potentially pending result of an asynchronous computation, Async provides a high-level monadic datatype \lstinline{Deferred.t}. With monadic composition, either using \lstinline{Deferred.bind} directly or some syntactic sugar for it, these asynchronous computations can be chained together.

For asynchronous computations to be executed, control has to be handed over to the Async scheduler using \lstinline{Scheduler.go}.

\paragraph*{Ivar \& upon}

While \lstinline{Deferred} provides a high-level way to compose existing asynchronous operations, to implement asynchronous operations from scratch, a low-level interface is provided via \lstinline{Ivar} and \lstinline{upon}.

An \lstinline{Ivar.t} is a variable which represents the actual value behind a Deferred, which can be manually filled at any time, prompting the Deferred to become determined. How and when the Ivar is filled can be arbitrary implemented, allowing for custom asynchronous operations. For this purpose, there also is a function \lstinline{upon} provided, which allows for scheduling arbitrary non-asynchronous code to be executed once a certain Deferred becomes determined.

As an example \cite{Concurre6:online}:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Caml,caption={Delayer example},captionpos=t]
module Delayer = struct
  type t = { delay: Time.Span.t;
             jobs: (unit -> unit) Queue.t;
           }

  let create delay =
    { delay; jobs = Queue.create () }

  let schedule t thunk =
    let ivar = Ivar.create () in
    Queue.enqueue t.jobs (fun () ->
      upon (thunk ()) (fun x -> Ivar.fill ivar x));
    upon (after t.delay) (fun () ->
      let job = Queue.dequeue_exn t.jobs in
      job ());
    Ivar.read ivar
end;;
\end{lstlisting}
\end{minipage}

This somewhat artificially conceived utility allows for scheduling some asynchronous operations to be executed in order, but after some predefined delay of time. Here, an Ivar is used to back the Deferred that is returned from the \lstinline{schedule} function. This Ivar is filled once, first, the artificial delay has elapsed (\lstinline{upon (after t.delay) (...)}), and then, the actual asynchronous operation that has been scheduled has become determined (\lstinline{upon (thunk ()) (...)}).

The monadic bind operator used to compose asynchronous operations can very simply be implemented utilizing IVar and \lstinline{upon} \cite{Concurre6:online}:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Caml,caption={Implementation of bind},captionpos=t]
let bind d ~f =
let i = Ivar.create () in
upon d (fun x -> upon (f x) (fun y -> Ivar.fill i y));
Ivar.read i;;
\end{lstlisting}
\end{minipage}

\section{Rust}
\label{sec:rust}

While Rust provides as part of it's compiler implementation and it's standard library a framework for encoding asynchronous tasks, it does not provide a runtime to execute these tasks. Therefore, while the core principles of asynchronous execution are defined by Rust itself, the details of how async tasks are executed is defined independently by different libraries \cite{TheAsync92:online}.

\paragraph*{Future}

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Rust,caption={Definition of Future},captionpos=t]
pub trait Future {
    type Output;

    fn poll(self: Pin<&mut Self>,
            cx: &mut Context<'_>) -> Poll<Self::Output>;
}
\end{lstlisting}
\end{minipage}

Provided as a trait by the Rust standard library, a Future represents an asynchronous computation that either is done, providing a final result value of the computation, or is still pending, needing to make further progress.

To invoke progress to be made on a Future, it has to be polled, which can either result in the Future completing it's computation, returning the final value, or in it suspending it's computation, ensuring by some way for the wakeup function that is passed in via the calling context \lstinline{cx} to be called once the Future is ready to be polled again for further progress.

Any \lstinline{struct} implementing Future therefore has to define in it's \lstinline{poll} both what computation is to be done to further it's progress in computing it's final value, and, if not complete, by what means the wakeup callback is to be called back to signify to the runtime that the Future can be polled again.

When and how any Future is polled is not defined by the Rust compiler or standard library, but instead rather can be handled differently by different external runtimes.

\paragraph*{Syntax}

To simplify declaration and composition of asynchronous tasks, the Rust compiler, since the 2018 edition, provides some built-in syntax for handling Futures.

\paragraph*{Async \& Await}

Manually constructing the necessary \lstinline{struct} and implementing \lstinline{Future} for it for every piece of asynchronous code is tedious, error prone and redundant. Therefore, the Rust compiler provides a way to declare async functions, closures and code blocks using the special keyword \lstinline{async}, from which during compilation the necessary implementation of \lstinline{Future} is generated \cite{asyncRus3:online}.

Similarly, instead of manually dealing with contexts and polling a Future directly, a keyword \lstinline{await} is provided by the Rust compiler to wait for a Future to conclude and extract it's resulting value \cite{Awaitexp35:online}.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Rust,caption={Example for async \& await},captionpos=t]
async fn foo() -> u8 { 5 }

fn bar() -> impl Future<Output = u8> {
    async {
        let x: u8 = foo().await;
        x + 5
    }
}
\end{lstlisting}
\end{minipage}

The code inside the async function, closure or code block will become the computation that is to be done to further the computation of the Future it compiles to, wherein any \lstinline{await} will become a point from which computation will be resumed when the asynchronous task that is being awaited concludes. For this, any data necessary to continue further computation will be stored in the implicitly generated struct that represents our Future.

This combination of \lstinline{async} and \lstinline{await} allows us write and compose asynchronous functions, closures and code blocks in a simple and general, runtime-independent manner.

\paragraph*{Join}

While we can dispatch and wait for a single asynchronous task with \lstinline{await}, if we want to dispatch multiple asynchronous tasks at once, awaiting each in turn will not suffice, since the execution of the calling async context will be suspended with the first \lstinline{await}, therefore resulting in the second asynchronous task to be initiated only once the first one has concluded, \&c. This means, that the asynchronous tasks will not be dispatched simultaneously, but rather each in sequence, unnecessarily leaving performance that could be gained with an asynchronous execution model on the table.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Rust,caption={Sequential await},captionpos=t]
async fn one() -> usize { 1 }
async fn two() -> usize { 2 }

async fn in_sequence() {
    let x = one().await;
    let y = two().await;
    assert_eq!((x, y), (1, 2));
}
\end{lstlisting}
\end{minipage}

To remedy this, the Rust standard library provides a macro for joining up multiple Futures, such that they are all polled together when awaited.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Rust,caption={Joined await},captionpos=t]
async fn one() -> usize { 1 }
async fn two() -> usize { 2 }

async fn in_sequence() {
    let (x,y) = join!(one(),two()).await;
    assert_eq!((x, y), (1, 2));
}
\end{lstlisting}
\end{minipage}

\paragraph*{Runtime}

The Rust standard library does not provide an async runtime, meaning that while we can readily declare and compose async functions, we can not actually execute any asynchronous code without the use of an external runtime. Neither does the standard library provide asynchronous versions of IO/network/file/\&c utilities. Instead, other than the basic interface described above of \lstinline{Future}, \lstinline{async} and \lstinline{await}, all further asynchronous functionality is implemented by external libraries \cite{TheAsync92:online}. Of these, Tokio is by far the most popular, and commonly considered the default choice for the majority of use cases \cite{AsyncRus83:online}.

\paragraph*{Tokio}

The primary runtime provided by Tokio utilizes a fixed sized pool of system-threads, usually aligned with the number of CPU cores available. To each such thread, called a Processor, asynchronous Tasks are non-preemptively scheduled to be executed \cite{Makingth23:online}. Alternatively, a single-threaded runtime is provided that uses only the currently running thread directly \cite{tokioexe87:online}.

To minimize necessary synchronization between Processors, instead of having one queue of Tasks from which all Processors take, each Processor has it's own dedicated queue of Tasks. When a Task spawns further Tasks, they are simply pushed onto the queue of the Processor which runs the Task. When a Processor runs out of tasks, it will attempt to steal Tasks from the queues of other Processors. When a Processor's queue becomes unproportionally full, idle Processors are woken up to steal some of it's load. This way, synchronization between Processors is minimized during normal operation under high load, while minimizing the impact on performance of an non-uniform distribution of load across Processors \cite{Makingth23:online}.

In parallel to the fixed size pool of threads used for most asynchronous operations, Tokio provides a second pool threads of variable but bound size for blocking or slow operations \cite{AsyncRus83:online}. This is necessary due to the non-preemptive scheduling of the primary pool of Processors. If a Task running on the primary pool takes too much time without yielding execution, other Tasks ready for execution will have to wait, impacting overall performance. This separation allows for the primary Processor pool to be optimized for fast switching of quickly yielding Tasks, while at the same time being able to cleanly interoperate with blocking or slow operations.

In addition to the runtime, Tokio also provides a suite of asynchronous IO, networking and file-system operations that mirror the synchronous versions provided by the Rust standard library.

\paragraph*{Usage}

The following implements a basic web server that asynchronously handles incoming connections \cite{spawnint88:online}:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Rust,caption={A basic async webserver},captionpos=t]
use tokio::net::{TcpListener, TcpStream};
use std::io;

async fn process(socket: TcpStream) {
    // ...
}

#[tokio::main]
async fn main() -> io::Result<()> {
    let listener = TcpListener::bind("127.0.0.1:8080").await?;

    loop {
        let (socket, _) = listener.accept().await?;

        tokio::spawn(
            async move {
            // Process each socket concurrently.
            process(socket).await
        });
    }
}
\end{lstlisting}
\end{minipage}

With the asynchronous version of \lstinline{TcpListener} provided by Tokio, the program waits asynchronously for incoming connections and spawns a new asynchronous task for each connected client without blocking the main task.

Notably, the \lstinline{#[tokio::main]} attribute makes it such that when the \lstinline{main} function is called (i.e. at the start of the program), a Tokio runtime is created and started, without requiring so to be done manually \cite{maininto61:online}.

\section{Haskell}
\label{sec:haskell}

All side-effecting computation in Haskell is represented with the monad \newline\lstinline{type IO a = World -> (a, World)}, where \lstinline{World} stands for a theoretical type that represents the entirety of the persistent outside world to our program\cite{jones2001monadic}. A value of this monad therefore stands for an action that transforms the current state of the world in some way (e.g. write to a file) and produces some internal result of type \lstinline{a} (e.g. the result of reading a file). These actions can be sequentially composed with the monad operators \lstinline{>>= :: IO a -> (a -> IO b) -> IO b} and the \lstinline{>> :: IO a -> IO b -> IO b}.

For an action represented by the IO monad to be actually executed and the side-effects they represent to occur, we assign this action to the special label \lstinline{main :: IO ()}. E.g.:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Haskell,caption={The main function in Haskell},captionpos=t]
main :: IO ()
main = getLine >>= \l -> putStr l
\end{lstlisting}
\end{minipage}

Here \lstinline{getLine :: IO String} and \lstinline{putStr :: String -> IO ()} are IO actions provided by the standard library, which in turn are implemented with simpler IO actions, down to a set of primitive IO actions that actually have to interact with Haskell-external non-pure systems, e.g. the kernel.

Notably this representation of side-effects through the chaining of IO actions deliberately considers the world and our actions upon it as strictly sequential. While there is no guarantee what effects our actions will have, it is guaranteed by Haskell that these actions happen in exactly the order specified. This is generally desired and a necessary distinction from normal lazy evaluation in Haskell, providing us the ability to declare needed precedence between our actions for correct interaction with the outside world (e.g. we can't receive a response before sending a query). However this strict ordering is fundamentally incompatible with concurrency, where we would explicitly like for actions to possibly happen in independent arbitrary order to facilitate asynchronous computation.

\paragraph*{Concurrent Haskell}

For this, an extension to both the language of Haskell and in turn the semantics of \lstinline{IO} is provided with \lstinline{forkIO :: IO () -> IO ThreadId}\cite{jones1996concurrent}. \lstinline{forkIO} takes an IO action and causes it to be executed concurrently to the parent thread from which it was called. Importantly, this means that whatever IO action is passed to \lstinline{forkIO} will no longer be executed in the ordered specified by the chain of IO actions of the parent thread. Any subsequent IO actions performed by the parent thread will be arbitrarily interleaved with IO actions performed by the child thread. The ordering of IO actions is only guaranteed internal to each thread respectively. For example:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Haskell,caption={Two threads running forever in parallel},captionpos=t]
forkIO (forever (putChar '1')) >> forever (putChar '0')
\end{lstlisting}
\end{minipage}

This will print an arbitrary mix of 0s and 1s to the output, with no determined order to them.

\paragraph*{Cross-thread communication with MVar}

While \lstinline{forkIO} allows to spawn a child thread to do IO actions on it's own, once forked, there is no way for the parent thread and child thread to communicate, neither can any two child thread communicate with each other\footnote{In GHC, forkIO returns a \lstinline{ThreadId}, with which the thread can be killed from the parent thread with \lstinline{killThread :: ThreadId -> IO ()}, which could be considered a form of communication.}. This is remedied by the introduction of \lstinline{data MVar a}, a possibly empty mutable location that is safely synchronized between threads\cite{jones1996concurrent}. A \lstinline{MVar} is created with \lstinline{newEmptyMVar :: IO (MVar a)} or \lstinline{newMVar :: a -> IO (MVar a)}, can be written to with \lstinline{putMVar :: MVar a -> a -> IO ()} and read from with \lstinline{takeMVar :: MVar a -> IO a}\cite{jones1996concurrent}. These latter two operations each block, \lstinline{putMVar} waiting for the \lstinline{MVar} to become empty, before writing to it, leaving it full, and \lstinline{takeMVar} waiting for the \lstinline{MVar} to become filled before reading it, leaving it empty\footnote{In the original definition of Concurrent Haskell, \lstinline{putMVar} did not block if the MVar is already full, but rather resulted in a program error. This however is changed in the present day GHC implementation.\cite{jones2001monadic}}. E.g.:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Haskell,caption={Using an MVar to hand a result back to the parent thread},captionpos=t]
do
  v <- newEmptyMVar
  forkIO (threadDelay 1000000 >> putMVar v "momentous result")
  m <- takeMVar v
  print m
\end{lstlisting}
\end{minipage}

This will cause the parent thread to wait for the child thread to fill \lstinline{v}, before printing the result that has been handed back to it.

\paragraph*{Async in Haskell}

While \lstinline{forkIO} provides a very general way for concurrent execution in Haskell, it has, as perhaps noticeable above, three properties that we would like to abstract over for asynchronous computation.

The first being, that the IO action we are forking does not have any result, the argument to \lstinline{forkIO} being of type \lstinline{IO ()}. Instead, to hand back a possible result to our parent thread, we have to manually create an \lstinline{MVar} in the parent thread and capture it in the forked IO action for the child to be able to hand back a result. 

The second being that the child thread can silently crash with an exception and leave our parent thread running none the wiser\cite{ControlC47:online}.

And the third being that, once spawned, our child thread might run indefinitely, even after our parent thread is gone, since it's execution becomes entirely independent of the parent\cite{ControlC47:online}.

All of these concerns are alleviated by the thin abstraction over Concurrent Haskell provided by the package \lstinline{async}.

An asynchronous thread is forked from the parent thread with \newline\lstinline{async :: IO a -> IO (Async a)}, and the parent thread can wait for the child thread to conclude it's computation with \lstinline{wait :: Async a -> IO a}\cite{ControlC47:online}. If an exception occurs in the child thread, \lstinline{wait} will cause this exception to be re-thrown in the parent thread, preventing silently unhandled exceptions. E.g.:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Haskell,caption={Fetching two pages asynchronously and waiting for both results},captionpos=t]
do
  a1 <- async (getURL url1)
  a2 <- async (getURL url2)
  page1 <- wait a1
  page2 <- wait a2
  print page1
  print page2
\end{lstlisting}
\end{minipage}

Notably, the child thread executing the IO action handed to \lstinline{async} will be started right away, and not just when awaited. Therefore, waiting first for \lstinline{a1} and then \lstinline{a2}, contrary to e.g. \lstinline{await} in Rust or \lstinline{Deferred.bind} in OCaml, does not result in the execution of the second asynchronous operation to be delayed until the first concludes, but rather they will be truly executed concurrently, even when awaited sequentially.

Unfortunately this also means that, while we have fulfilled both the first and second wish above, if used in this style, the third issue is still standing; Any child threads will keep running, even if the parent thread dies, since the execution of the child thread does not depend on the parent thread to be waiting for it\cite{ControlC47:online}. This usually is unwanted. Therefore, the function \lstinline{withAsync :: IO a -> (Async a -> IO b) -> IO b} is provided, which will asynchronously run the given IO action and hand the \lstinline{Async} into the given function, while also keeping track of the child thread that has been forked off, and cancelling it when the given function returns or throws an exception, ensuring that no orphaned child thread is left running. E.g.:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Haskell,caption={Using withAsync to prevent orphaned threads},captionpos=t]
withAsync (getURL url1) $ \a1 -> do
  withAsync (getURL url2) $ \a2 -> do
    page1 <- wait a1
    page2 <- wait a2
    print page1
    print page2
\end{lstlisting}
\end{minipage}

Abstracting again over \lstinline{withAsync}, a few high-level utilities are provided, like \newline\lstinline{concurrently :: IO a -> IO b -> IO (a, b)} or \lstinline{race :: IO a -> IO b -> IO (Either a b)}\cite{ControlC47:online}. Using \lstinline{concurrently}, the above example can be rewritten as:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Haskell,caption={Simplifying the example with concurrently},captionpos=t]
do
  (page1, page2) <- concurrently (getURL url1) (getURL url2)
  print page1
  print page2
\end{lstlisting}
\end{minipage}

\lstinline{race} provides the possibility not to wait for both of two asynchronous operations to conclude, but rather to wait for either to conclude and the other to subsequently be canceled. E.g.:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Haskell,caption={Racing two fetches and printing whichever responds first},captionpos=t]
do
  r <- race (getURL url1) (getURL url2)
  case r of
    Left page -> print page
    Right page -> print page
\end{lstlisting}
\end{minipage}

\paragraph*{Runtime}

In the Glasgow Haskell Compiler (GHC), concurrent threads are represented by as system of internal lightweight threads which are preemptively scheduled by a custom scheduler\cite{schedule78:online}. By default, this occurs entirely on one system thread, but an option is provided for utilizing multiple CPU cores via SMP parallelism\cite{54UsingC70:online}.

%
% Bibliography
%

% Please use bibtex, 

\bibliography{lipics-v2021-sample-article}

\appendix

\end{document}
