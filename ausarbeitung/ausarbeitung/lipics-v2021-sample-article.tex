\documentclass[a4paper,UKenglish,cleveref, autoref, thm-restate]{lipics-v2021}

\usepackage{listings-rust}
%This is a template for producing LIPIcs articles. 
%See lipics-v2021-authors-guidelines.pdf for further information.
%for A4 paper format use option "a4paper", for US-letter use option "letterpaper"
%for british hyphenation rules use option "UKenglish", for american hyphenation rules use option "USenglish"
%for section-numbered lemmas etc., use "numberwithinsect"
%for enabling cleveref support, use "cleveref"
%for enabling autoref support, use "autoref"
%for anonymousing the authors (e.g. for double-blind review), add "anonymous"
%for enabling thm-restate support, use "thm-restate"
%for enabling a two-column layout for the author/affilation part (only applicable for > 6 authors), use "authorcolumns"
%for producing a PDF according the PDF/A standard, add "pdfa"

%\pdfoutput=1 %uncomment to ensure pdflatex processing (mandatatory e.g. to submit to arXiv)
\hideLIPIcs  %uncomment to remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository

%\graphicspath{{./graphics/}}%helpful if your graphic files are in another directory

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Approaches to Asynchronous Execution across Languages}

%\titlerunning{Dummy short title} %TODO optional, please use if title is longer than one line

\author{Tobias Hoffmann}{Albert-Ludwigs-Universit√§t Freiburg, Germany}{garbaz@t-online.de}{}{}

\authorrunning{T. Hoffmann} %TODO mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{Tobias Hoffmann} %TODO mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\begin{CCSXML}
<ccs2012>
    <concept>
        <concept_id>10003752.10003753.10003761.10003762</concept_id>
        <concept_desc>Theory of computation~Parallel computing models</concept_desc>
        <concept_significance>500</concept_significance>
    </concept>
</ccs2012>
\end{CCSXML}
    
\ccsdesc[500]{Theory of computation~Parallel computing models}

\keywords{async, concurrency} %TODO mandatory; please add comma-separated list of keywords

\category{} %optional, e.g. invited paper

\relatedversion{} %optional, e.g. full version hosted on arXiv, HAL, or other respository/website
%\relatedversiondetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93]{Classification (e.g. Full Version, Extended Version, Previous Version}{URL to related version} %linktext and cite are optional

%\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
%\supplementdetails[linktext={opt. text shown instead of the URL}, cite=DBLP:books/mk/GrayR93, subcategory={Description, Subcategory}, swhid={Software Heritage Identifier}]{General Classification (e.g. Software, Dataset, Model, ...)}{URL to related version} %linktext, cite, and subcategory are optional

%\funding{(Optional) general funding statement \dots}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

\acknowledgements{}%optional

\nolinenumbers %uncomment to disable line numbering



%Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \EventEditors{John Q. Open and Joan R. Access}
% \EventNoEds{2}
% \EventLongTitle{42nd Conference on Very Important Topics (CVIT 2016)}
% \EventShortTitle{CVIT 2016}
% \EventAcronym{CVIT}
% \EventYear{2016}
% \EventDate{December 24--27, 2016}
% \EventLocation{Little Whinging, United Kingdom}
% \EventLogo{}
% \SeriesVolume{42}
% \ArticleNo{0}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\maketitle

%TODO mandatory: add short abstract of the document
\begin{abstract}
    In computing operations can be divided into either being CPU-bound or being IO-bound, the former being operations where the limiting factor to performance is the speed with which calculations can be performed, whereas the latter are operations where performance is limited by the responses of systems external to the CPU. While for both kinds of operations, concurrency is a commonly taken approach to achieve improvements in performance, the way this concurrency is systematically structured and implemented is quite different for the two applications. In the following, I will focus on the approaches provided by a selection of different languages to efficiently handle specifically IO-bound operations.
\end{abstract}

\section{Overview}
\label{sec:overview}

The primary distinction of IO-bound tasks is that they generally consist of a small amount of computation, while having relatively long wait times. This means that each IO-bound task does not require much CPU time in total, but importantly, whenever an IO-bound task does have to do computation, it is usually important for it to get the necessary CPU time quickly, so that it can dispatch another message and wait for a response from whatever external system it is interacting with.

So to handle a large number of such tasks simultaneously requires quick and efficient switching of tasks to fully utilize the processing resources available. However, ordinary threads as provided by most operating systems generally require too much time and resources to be created and deleted for them to be utilized directly. Therefore, most systems optimized for asynchronous execution of IO-bound tasks utilize a custom system that is abstracted over, or even entirely independent from, the concurrency systems provided by the underlying system architecture.

Another challenge in developing a system for asynchronous computation is the matter of a usable and safe abstract interface for programmers to interact with, due to it being such a fundamental change in how computation is organized inside a program. For this, two different approaches have developed, which are semantically similar, but conceptually and principally distinct.

One, generally aligned with the imperative programming paradigm, is to consider an asynchronous sequence of computation as one whole, which is then routinely interrupted and later resumed whenever it has to wait from some external asynchronous operation. This is usually implemented with bespoke features introduced into the language itself.

The other, generally aligned with the functional programming paradigm, is to instead consider an asynchronous sequence of computation to be made up of a chain of smaller asynchronous operations, resulting in a monadic structure where smaller asynchronous operations are composed into larger asynchronous operations.

While these two approaches look at the problem from different sides, in effect they result in the same semantic model: Bursts of computation delineated by waiting for external responses.

\section{Ocaml}
\label{sec:ocaml}

While neither the OCaml compiler nor the standard library \verb|Base| provide any support for asynchronous computation, there are external libraries that introduce the functionality into the language. One is \verb|Async|, which comes with Jane Street's \verb|Core| library, another is \verb|Lwt|.

\paragraph*{Async}

Since the basic ideas of this library have already been covered in the presentation, their explanation will be kept brief here.

Async does not utilize system threads for concurrency, but rather implements it's own system of non-preemptive user-level threads to handle asynchronous operations \cite{Concurre6:online}. While this means that it does not benefit from the potential increase in performance from utilizing multiple cores, it does avoid the overhead that comes with creating and destroying system threads, while still allowing the user to avoid dead-time when waiting for external responses.

\paragraph*{Deferred}

For representing the potentially pending result of an asynchronous computation, Async provides a high-level monadic datatype \verb|Deferred.t|. With monadic composition, either using \verb|Deferred.bind| directly or some syntactic sugar for it, these asynchronous computations can be chained together.

For asynchronous computations to be executed, control has to be handed over to the Async scheduler using \verb|Scheduler.go|.

\paragraph*{Ivar \& upon}

While \verb|Deferred| provides a high-level way to compose existing asynchronous operations, to implement asynchronous operations from scratch, a low-level interface is provided via \verb|Ivar| and \verb|upon|.

An \verb|Ivar.t| is a variable which represents the actual value behind a Deferred, which can be manually filled at any time, prompting the Deferred to become determined. How and when the Ivar is filled can be arbitrary implemented, allowing for custom asynchronous operations. For this purpose, there also is a function \verb|upon| provided, which allows for scheduling arbitrary non-asynchronous code to be executed once a certain Deferred becomes determined.

As an example \cite{Concurre6:online}:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Caml,caption={Delayer example},captionpos=t]
module Delayer = struct
  type t = { delay: Time.Span.t;
             jobs: (unit -> unit) Queue.t;
           }

  let create delay =
    { delay; jobs = Queue.create () }

  let schedule t thunk =
    let ivar = Ivar.create () in
    Queue.enqueue t.jobs (fun () ->
      upon (thunk ()) (fun x -> Ivar.fill ivar x));
    upon (after t.delay) (fun () ->
      let job = Queue.dequeue_exn t.jobs in
      job ());
    Ivar.read ivar
end;;
\end{lstlisting}
\end{minipage}

This somewhat artificially conceived utility allows for scheduling some asynchronous operations to be executed in order, but after some predefined delay of time. Here, an Ivar is used to back the Deferred that is returned from the \verb|schedule| function. This Ivar is filled once, first, the artificial delay has elapsed (\verb|upon (after t.delay) (...)|), and then, the actual asynchronous operation that has been scheduled has become determined (\verb|upon (thunk ()) (...)|).

The monadic bind operator used to compose asynchronous operations can very simply be implemented utilizing IVar and \verb|upon| \cite{Concurre6:online}:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Caml,caption={Implementation of bind},captionpos=t]
let bind d ~f =
let i = Ivar.create () in
upon d (fun x -> upon (f x) (fun y -> Ivar.fill i y));
Ivar.read i;;
\end{lstlisting}
\end{minipage}

\section{Rust}
\label{sec:rust}

While Rust provides as part of it's compiler implementation and it's standard library a framework for encoding asynchronous tasks, it does not provide a runtime to execute these tasks. Therefore, while the core principles of asynchronous execution are defined by Rust itself, the details of how async tasks are executed is defined independently by different libraries \cite{TheAsync92:online}.

\paragraph*{Future}

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Rust,caption={Definition of Future},captionpos=t]
pub trait Future {
    type Output;

    fn poll(self: Pin<&mut Self>,
            cx: &mut Context<'_>) -> Poll<Self::Output>;
}
\end{lstlisting}
\end{minipage}

Provided as a trait by the Rust standard library, a Future represents an asynchronous computation that either is done, providing a final result value of the computation, or is still pending, needing to make further progress.

To invoke progress to be made on a Future, it has to be polled, which can either result in the Future completing it's computation, returning the final value, or in it suspending it's computation, ensuring by some way for the wakeup function that is passed in via the calling context \verb|cx| to be called once the Future is ready to be polled again for further progress.

Any \verb|struct| implementing Future therefore has to define in it's \verb|poll| both what computation is to be done to further it's progress in computing it's final value, and, if not complete, by what means the wakeup callback is to be called back to signify to the runtime that the Future can be polled again.

When and how any Future is polled is not defined by the Rust compiler or standard library, but instead rather can be handled differently by different external runtimes.

\paragraph*{Syntax}

To simplify declaration and composition of asynchronous tasks, the Rust compiler, since the 2018 edition, provides some built-in syntax for handling Futures.

\paragraph*{Async \& Await}

Manually constructing the necessary \verb|struct| and implementing \verb|Future| for it for every piece of asynchronous code is tedious, error prone and redundant. Therefore, the Rust compiler provides a way to declare async functions, closures and code blocks using the special keyword \verb|async|, from which during compilation the necessary implementation of \verb|Future| is generated \cite{asyncRus3:online}.

Similarly, instead of manually dealing with contexts and polling a Future directly, a keyword \verb|await| is provided by the Rust compiler to wait for a Future to conclude and extract it's resulting value \cite{Awaitexp35:online}.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Rust,caption={Example for async \& await},captionpos=t]
async fn foo() -> u8 { 5 }

fn bar() -> impl Future<Output = u8> {
    async {
        let x: u8 = foo().await;
        x + 5
    }
}
\end{lstlisting}
\end{minipage}

The code inside the async function, closure or code block will become the computation that is to be done to further the computation of the Future it compiles to, wherein any \verb|await| will become a point from which computation will be resumed when the asynchronous task that is being awaited concludes. For this, any data necessary to continue further computation will be stored in the implicitly generated struct that represents our Future.

This combination of \verb|async| and \verb|await| allows us write and compose asynchronous functions, closures and code blocks in a simple and general, runtime-independent manner.

\paragraph*{Join}

While we can dispatch and wait for a single asynchronous task with \verb|await|, if we want to dispatch multiple asynchronous tasks at once, awaiting each in turn will not suffice, since the execution of the calling async context will be suspended with the first \verb|await|, therefore resulting in the second asynchronous task to be initiated only once the first one has concluded, \&c. This means, that the asynchronous tasks will not be dispatched simultaneously, but rather each in sequence, unnecessarily leaving performance that could be gained with an asynchronous execution model on the table.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Rust,caption={Sequential await},captionpos=t]
async fn one() -> usize { 1 }
async fn two() -> usize { 2 }

async fn in_sequence() {
    let x = one().await;
    let y = two().await;
    assert_eq!((x, y), (1, 2));
}
\end{lstlisting}
\end{minipage}

To remedy this, the Rust standard library provides a macro for joining up multiple Futures, such that they are all polled together when awaited.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Rust,caption={Joined await},captionpos=t]
async fn one() -> usize { 1 }
async fn two() -> usize { 2 }

async fn in_sequence() {
    let (x,y) = join!(one(),two()).await;
    assert_eq!((x, y), (1, 2));
}
\end{lstlisting}
\end{minipage}

\paragraph*{Runtime}

The Rust standard library does not provide an async runtime, meaning that while we can readily declare and compose async functions, we can not actually execute any asynchronous code without the use of an external runtime. Neither does the standard library provide asynchronous versions of IO/network/file/\&c utilities. Instead, other than the basic interface described above of \verb|Future|, \verb|async| and \verb|await|, all further asynchronous functionality is implemented by external libraries \cite{TheAsync92:online}. Of these, Tokio is by far the most popular, and commonly considered the default choice for the majority of use cases \cite{AsyncRus83:online}.

\paragraph*{Tokio}

The primary runtime provided by Tokio utilizes a fixed sized pool of system-threads, usually aligned with the number of CPU cores available. To each such thread, called a Processor, asynchronous Tasks are non-preemptively scheduled to be executed \cite{Makingth23:online}. Alternatively, a single-threaded runtime is provided that uses only the currently running thread directly \cite{tokioexe87:online}.

To minimize necessary synchronization between Processors, instead of having one queue of Tasks from which all Processors take, each Processor has it's own dedicated queue of Tasks. When a Task spawns further Tasks, they are simply pushed onto the queue of the Processor which runs the Task. When a Processor runs out of tasks, it will attempt to steal Tasks from the queues of other Processors. When a Processor's queue becomes unproportionally full, idle Processors are woken up to steal some of it's load. This way, synchronization between Processors is minimized during normal operation under high load, while minimizing the impact on performance of an non-uniform distribution of load across Processors \cite{Makingth23:online}.

In parallel to the fixed size pool of threads used for most asynchronous operations, Tokio provides a second pool threads of variable but bound size for blocking or slow operations \cite{AsyncRus83:online}. This is necessary due to the non-preemptive scheduling of the primary pool of Processors. If a Task running on the primary pool takes too much time without yielding execution, other Tasks ready for execution will have to wait, impacting overall performance. This separation allows for the primary Processor pool to be optimized for fast switching of quickly yielding Tasks, while at the same time being able to cleanly interoperate with blocking or slow operations.

In addition to the runtime, Tokio also provides a suite of asynchronous IO, networking and file-system operations that mirror the synchronous versions provided by the Rust standard library.

\paragraph*{Usage}

The following implements a basic web server that asynchronously handles incoming connections \cite{spawnint88:online}:

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=Rust,caption={A basic async webserver},captionpos=t]
use tokio::net::{TcpListener, TcpStream};
use std::io;

async fn process(socket: TcpStream) {
    // ...
}

#[tokio::main]
async fn main() -> io::Result<()> {
    let listener = TcpListener::bind("127.0.0.1:8080").await?;

    loop {
        let (socket, _) = listener.accept().await?;

        tokio::spawn(
            async move {
            // Process each socket concurrently.
            process(socket).await
        });
    }
}
\end{lstlisting}
\end{minipage}

With the asynchronous version of \verb|TcpListener| provided by Tokio, the program waits asynchronously for incoming connections and spawns a new asynchronous task for each connected client without blocking the main task.

Notably, the \verb|#[tokio::main]| attribute makes it such that when the \verb|main| function is called (i.e. at the start of the program), a Tokio runtime is created and started, without requiring so to be done manually \cite{maininto61:online}.


%
% Bibliography
%

% Please use bibtex, 

\bibliography{lipics-v2021-sample-article}

\appendix

\end{document}
